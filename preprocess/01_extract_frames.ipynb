{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess video  \n",
        "### Download video and split it into frames  \n",
        "- NOTE: SAM2 assumee that the video is stored as a list of JPEG frames with filenames like <frame_index>.jpg. Let's start by downloading a sample video, splitting it into frames, and saving them to disk. Feel free to replace SOURCE_VIDEO with the path to your video file.\n",
        "- 비디오를 다운로드하여 프레임으로 나누고 디스크에 저장   "
      ],
      "metadata": {
        "id": "sMMurPvZTw-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup 및 라이브러리 설치  "
      ],
      "metadata": {
        "id": "_m5tB-_ASa_u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApuR1MwRta8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d6f04ed-eeeb-4fa3-e3a2-4e05100382d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HOME: /content\n"
          ]
        }
      ],
      "source": [
        "# NOTE: To make it easier for us to manage datasets, images and models we create a HOME constant.\n",
        "import os\n",
        "\n",
        "HOME = os.getcwd()\n",
        "print(\"HOME:\", HOME)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install SAM2 and dependencies\n",
        "!git clone https://github.com/facebookresearch/segment-anything-2.git\n",
        "%cd {HOME}/segment-anything-2\n",
        "!pip install -e . -q\n",
        "!python setup.py build_ext --inplace\n",
        "\n",
        "# 필수 라이브러리 설치\n",
        "!pip install -q supervision[assets] jupyter_bbox_widget\n",
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "uN6YzVhitlYx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "c9e934a6-4ab8-4895-aca8-c921fc073a18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'segment-anything-2'...\n",
            "remote: Enumerating objects: 1070, done.\u001b[K\n",
            "remote: Total 1070 (delta 0), reused 0 (delta 0), pack-reused 1070 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1070/1070), 128.11 MiB | 13.87 MiB/s, done.\n",
            "Resolving deltas: 100% (381/381), done.\n",
            "/content/segment-anything-2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building editable for SAM-2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "running build_ext\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:529: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:448: UserWarning: The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.4). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:458: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.5\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "building 'sam2._C' extension\n",
            "creating build/temp.linux-x86_64-cpython-311/sam2/csrc\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c sam2/csrc/connected_components.cu -o build/temp.linux-x86_64-cpython-311/sam2/csrc/connected_components.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "creating build/lib.linux-x86_64-cpython-311/sam2\n",
            "x86_64-linux-gnu-g++ -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions build/temp.linux-x86_64-cpython-311/sam2/csrc/connected_components.o -L/usr/local/lib/python3.11/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-311/sam2/_C.so\n",
            "copying build/lib.linux-x86_64-cpython-311/sam2/_C.so -> sam2\n",
            "\u001b[33mWARNING: supervision 0.25.1 does not provide the extra 'assets'\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.7/220.7 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.4/837.4 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAM2 체크포인트 다운로드 및 데이터 다운로드  "
      ],
      "metadata": {
        "id": "lSBsNjXGGLvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download SAM2 checkpoints\n",
        "!mkdir -p {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt -P {HOME}/checkpoints\n",
        "\n",
        "# yolo11s best model\n",
        "!gdown 1A6_iThtWBlzgvGqOP9QXuDYGFL809jYL"
      ],
      "metadata": {
        "id": "1DRz2Ljxtlc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라이브러리 Imports 및 모델 초기화  "
      ],
      "metadata": {
        "id": "VJ1f4XOHSSLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import base64\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "\n",
        "from pathlib import Path\n",
        "from supervision.assets import download_assets, VideoAssets\n",
        "from sam2.build_sam import build_sam2_video_predictor\n",
        "\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "IS_COLAB = True\n",
        "\n",
        "if IS_COLAB:\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n",
        "\n",
        "from jupyter_bbox_widget import BBoxWidget"
      ],
      "metadata": {
        "id": "xR62T-q5tvQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: This code enables mixed-precision computing for faster deep learning. It uses bfloat16 for most calculations and, on newer NVIDIA GPUs, leverages TensorFloat-32 (TF32) for certain operations to further boost performance.\n",
        "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "\n",
        "if torch.cuda.get_device_properties(0).major >= 8:\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "CHECKPOINT = f\"{HOME}/checkpoints/sam2_hiera_large.pt\"\n",
        "CONFIG = \"sam2_hiera_l.yaml\"\n",
        "\n",
        "# sam2 model 초기화\n",
        "sam2_model = build_sam2_video_predictor(CONFIG, CHECKPOINT)"
      ],
      "metadata": {
        "id": "Zt5jOlm0tlhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 비디오 정보 확인(fps, 프레임 수 등)"
      ],
      "metadata": {
        "id": "VBVjJUUBLZk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/content/test video 07.mp4\" # 사용할 비디오 경로\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # 원본 총 프레임 개수\n",
        "duration = frame_count / fps  # 계산된 비디오 길이\n",
        "cap.release()\n",
        "\n",
        "print(f\"비디오 FPS: {fps:.6f}\")\n",
        "print(f\"비디오 총 프레임 개수: {frame_count}\")\n",
        "print(f\"계산된 비디오 길이: {duration:.2f}초\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "pYwm2saaEwx1",
        "outputId": "c6c8588e-b87b-4f66-98b7-353559b5bfdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "float division by zero",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-918650330.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCAP_PROP_FPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mframe_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCAP_PROP_FRAME_COUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 원본 총 프레임 개수\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_count\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfps\u001b[0m  \u001b[0;31m# 계산된 비디오 길이\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 비디오 -> 프레임 분할 및 저장  \n",
        "- sv.get_video_frames_generator(SOURCE_VIDEO, start=0, end=None) 함수를 사용  \n",
        "- 1초에 몇 개의 프레임이 저장되었는지 확인 가능  "
      ],
      "metadata": {
        "id": "As4SLP9wDVrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SCALE_FACTOR = 0.5  # 이미지 크기 50% 축소\n",
        "SOURCE_VIDEO = \"/content/test video 07.mp4\"\n",
        "SOURCE_FRAMES = Path(HOME) / Path(SOURCE_VIDEO).stem\n",
        "SOURCE_FRAMES.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "frames_generator = sv.get_video_frames_generator(SOURCE_VIDEO, start=0, end=None)\n",
        "\n",
        "images_sink = sv.ImageSink(\n",
        "    target_dir_path=SOURCE_FRAMES.as_posix(),\n",
        "    overwrite=True,\n",
        "    image_name_pattern=\"{:05d}.jpg\"\n",
        ")\n",
        "\n",
        "with images_sink:\n",
        "    for frame in frames_generator:\n",
        "        frame = sv.scale_image(frame, SCALE_FACTOR)\n",
        "        images_sink.save_image(frame)\n",
        "\n",
        "# 첫번째 프레임 및 결과 및 저장 경로\n",
        "SOURCE_IMAGE = SOURCE_FRAMES / \"00000.jpg\"\n",
        "TARGET_VIDEO = Path(HOME) / f\"{Path(SOURCE_VIDEO).stem}-result-07.mp4\""
      ],
      "metadata": {
        "id": "_GusUAIZCz1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 프레임 압축\n",
        "folder_path = \"/content/test video 07\"  # 저장된 프레임이 있는 폴더\n",
        "zip_path = \"/content/test_video_frames_07.zip\"  # 압축 파일명\n",
        "\n",
        "shutil.make_archive(zip_path.replace(\".zip\", \"\"), 'zip', folder_path)\n",
        "files.download(zip_path)"
      ],
      "metadata": {
        "id": "SAQ1-_ts3JIE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "29c78b2c-bdbd-43db-9d38-c571a9cc3859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a7df851e-e59b-4250-be81-22f8894470ba\", \"test_video_frames_08.zip\", 21455621)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the inference state\n",
        "- NOTE: SAM 2 requires stateful inference for interactive video segmentation, so we need to initialize an inference state on this video. During initialization, it loads all the JPEG frames in video_path and stores their pixels in inference_state (as shown in the progress bar below).  \n",
        "- SAM 2는 비디오 인터랙티브 세그멘테이션을 위해 상태를 유지하는(stateful) 추론 방식을 사용하기 때문에, 비디오에 대한 추론을 실행하기 전에 먼저 \"추론 상태(inference state)\"를 초기화해야 합니다.\n",
        "- 초기화하는 동안, video_path에 있는 모든 JPEG 프레임을 불러와서 프레임의 픽셀 데이터를 inference_state에 저장합니다.  "
      ],
      "metadata": {
        "id": "sbQZyJ35cJqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAM2 모델 초기화 (inference 대비용)  "
      ],
      "metadata": {
        "id": "6jHLPJUUG4UP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- NOTE: 만약 inference_state를 사용하여 이전에 **트래킹(Tracking, 추적)**을 실행했다면, 먼저 reset_state()를 호출하여 초기화해야 합니다."
      ],
      "metadata": {
        "id": "bSg9yvgrduwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sam2 모델 초기화\n",
        "inference_state = sam2_model.init_state(video_path=SOURCE_FRAMES.as_posix())\n",
        "sam2_model.reset_state(inference_state)"
      ],
      "metadata": {
        "id": "YvHKatbQbkX1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "386555c5-7fc1-43d4-ed7f-b7cd50956c81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "frame loading (JPEG): 100%|██████████| 379/379 [00:11<00:00, 34.21it/s]\n"
          ]
        }
      ]
    }
  ]
}