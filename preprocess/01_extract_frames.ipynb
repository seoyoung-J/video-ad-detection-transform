{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess video  \n",
        "### Download video and split it into frames  \n",
        "- NOTE: SAM2 assumee that the video is stored as a list of JPEG frames with filenames like <frame_index>.jpg. Let's start by downloading a sample video, splitting it into frames, and saving them to disk. Feel free to replace SOURCE_VIDEO with the path to your video file.\n",
        "- 비디오를 다운로드하여 프레임으로 나누고 디스크에 저장   "
      ],
      "metadata": {
        "id": "sMMurPvZTw-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup 및 라이브러리 설치  "
      ],
      "metadata": {
        "id": "_m5tB-_ASa_u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApuR1MwRta8L"
      },
      "outputs": [],
      "source": [
        "# NOTE: To make it easier for us to manage datasets, images and models we create a HOME constant.\n",
        "import os\n",
        "\n",
        "HOME = os.getcwd()\n",
        "print(\"HOME:\", HOME)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install SAM2 and dependencies\n",
        "!git clone https://github.com/facebookresearch/segment-anything-2.git\n",
        "%cd {HOME}/segment-anything-2\n",
        "!pip install -e . -q\n",
        "!python setup.py build_ext --inplace\n",
        "\n",
        "# 필수 라이브러리 설치\n",
        "!pip install -q supervision[assets] jupyter_bbox_widget\n",
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "uN6YzVhitlYx",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAM2 체크포인트 다운로드 및 데이터 다운로드  "
      ],
      "metadata": {
        "id": "lSBsNjXGGLvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download SAM2 checkpoints\n",
        "!mkdir -p {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt -P {HOME}/checkpoints\n",
        "\n",
        "# yolo11s best model\n",
        "!gdown 1A6_iThtWBlzgvGqOP9QXuDYGFL809jYL"
      ],
      "metadata": {
        "id": "1DRz2Ljxtlc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라이브러리 Imports 및 모델 초기화  "
      ],
      "metadata": {
        "id": "VJ1f4XOHSSLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import base64\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "\n",
        "from pathlib import Path\n",
        "from supervision.assets import download_assets, VideoAssets\n",
        "from sam2.build_sam import build_sam2_video_predictor\n",
        "\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "IS_COLAB = True\n",
        "\n",
        "if IS_COLAB:\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n",
        "\n",
        "from jupyter_bbox_widget import BBoxWidget"
      ],
      "metadata": {
        "id": "xR62T-q5tvQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: This code enables mixed-precision computing for faster deep learning. It uses bfloat16 for most calculations and, on newer NVIDIA GPUs, leverages TensorFloat-32 (TF32) for certain operations to further boost performance.\n",
        "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "\n",
        "if torch.cuda.get_device_properties(0).major >= 8:\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "CHECKPOINT = f\"{HOME}/checkpoints/sam2_hiera_large.pt\"\n",
        "CONFIG = \"sam2_hiera_l.yaml\"\n",
        "\n",
        "# sam2 model 초기화\n",
        "sam2_model = build_sam2_video_predictor(CONFIG, CHECKPOINT)"
      ],
      "metadata": {
        "id": "Zt5jOlm0tlhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 비디오 정보 확인(fps, 프레임 수 등)"
      ],
      "metadata": {
        "id": "VBVjJUUBLZk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/content/test video 07.mp4\" # 사용할 비디오 경로\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # 원본 총 프레임 개수\n",
        "duration = frame_count / fps  # 계산된 비디오 길이\n",
        "cap.release()\n",
        "\n",
        "print(f\"비디오 FPS: {fps:.6f}\")\n",
        "print(f\"비디오 총 프레임 개수: {frame_count}\")\n",
        "print(f\"계산된 비디오 길이: {duration:.2f}초\")"
      ],
      "metadata": {
        "id": "pYwm2saaEwx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 비디오 -> 프레임 분할 및 저장  \n",
        "- sv.get_video_frames_generator(SOURCE_VIDEO, start=0, end=None) 함수를 사용  \n",
        "- 1초에 몇 개의 프레임이 저장되었는지 확인 가능  "
      ],
      "metadata": {
        "id": "As4SLP9wDVrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SCALE_FACTOR = 0.5  # 이미지 크기 50% 축소\n",
        "SOURCE_VIDEO = \"/content/test video 07.mp4\"\n",
        "SOURCE_FRAMES = Path(HOME) / Path(SOURCE_VIDEO).stem\n",
        "SOURCE_FRAMES.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "frames_generator = sv.get_video_frames_generator(SOURCE_VIDEO, start=0, end=None)\n",
        "\n",
        "images_sink = sv.ImageSink(\n",
        "    target_dir_path=SOURCE_FRAMES.as_posix(),\n",
        "    overwrite=True,\n",
        "    image_name_pattern=\"{:05d}.jpg\"\n",
        ")\n",
        "\n",
        "with images_sink:\n",
        "    for frame in frames_generator:\n",
        "        frame = sv.scale_image(frame, SCALE_FACTOR)\n",
        "        images_sink.save_image(frame)\n",
        "\n",
        "# 첫번째 프레임 및 결과 및 저장 경로\n",
        "SOURCE_IMAGE = SOURCE_FRAMES / \"00000.jpg\"\n",
        "TARGET_VIDEO = Path(HOME) / f\"{Path(SOURCE_VIDEO).stem}-result-07.mp4\""
      ],
      "metadata": {
        "id": "_GusUAIZCz1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 프레임 압축\n",
        "folder_path = \"/content/test video 07\"  # 저장된 프레임이 있는 폴더\n",
        "zip_path = \"/content/test_video_frames_07.zip\"  # 압축 파일명\n",
        "\n",
        "shutil.make_archive(zip_path.replace(\".zip\", \"\"), 'zip', folder_path)\n",
        "files.download(zip_path)"
      ],
      "metadata": {
        "id": "SAQ1-_ts3JIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the inference state\n",
        "- NOTE: SAM 2 requires stateful inference for interactive video segmentation, so we need to initialize an inference state on this video. During initialization, it loads all the JPEG frames in video_path and stores their pixels in inference_state (as shown in the progress bar below).  \n",
        "- SAM 2는 비디오 인터랙티브 세그멘테이션을 위해 상태를 유지하는(stateful) 추론 방식을 사용하기 때문에, 비디오에 대한 추론을 실행하기 전에 먼저 \"추론 상태(inference state)\"를 초기화해야 합니다.\n",
        "- 초기화하는 동안, video_path에 있는 모든 JPEG 프레임을 불러와서 프레임의 픽셀 데이터를 inference_state에 저장합니다.  "
      ],
      "metadata": {
        "id": "sbQZyJ35cJqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAM2 모델 초기화 (inference 대비용)  "
      ],
      "metadata": {
        "id": "6jHLPJUUG4UP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- NOTE: 만약 inference_state를 사용하여 이전에 **트래킹(Tracking, 추적)**을 실행했다면, 먼저 reset_state()를 호출하여 초기화해야 합니다."
      ],
      "metadata": {
        "id": "bSg9yvgrduwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sam2 모델 초기화\n",
        "inference_state = sam2_model.init_state(video_path=SOURCE_FRAMES.as_posix())\n",
        "sam2_model.reset_state(inference_state)"
      ],
      "metadata": {
        "id": "YvHKatbQbkX1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}