{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess video 과정  \n",
        "1. **환경 세팅**  \n",
        "- 작업 디렉토리(HOME) 및 장치(CPU/GPU) 설정\n",
        "- 필수 라이브러리(SAM2, YOLOv11) 설치\n",
        "- SAM2 체크포인트 및 YOLOv11 학습 모델 다운로드\n",
        "\n",
        "2. **sam2 모델 초기화**\n",
        "- CONFIG 및 CHECKPOINT 로드\n",
        "- SAM2 모델 불러오기 (build_sam2_video_predictor)\n",
        "\n",
        "3. **비디오 정보 확인**  \n",
        "- fps, 총 프레임 수, 비디오 길이 출력  \n",
        "\n",
        "4. **비디오 -> 프레임 분할 및 저장**  \n",
        "- 비디오를 프레임 이미지(JPEG)로 분리\n",
        "- 프레임 크기 50% 축소 후 디스크에 저장\n",
        "- 전처리된 프레임들을 ZIP 파일로 압축 및 다운로드  \n",
        "\n",
        "5. **sam2 초기화**  \n",
        "- 프레임들을 불러와 inference 상태 초기화\n",
        "- 기존 상태가 있으면 reset 후 초기화"
      ],
      "metadata": {
        "id": "sMMurPvZTw-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 환경 세팅  "
      ],
      "metadata": {
        "id": "_m5tB-_ASa_u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApuR1MwRta8L"
      },
      "outputs": [],
      "source": [
        "# NOTE: To make it easier for us to manage datasets, images and models we create a HOME constant.\n",
        "import os\n",
        "\n",
        "HOME = os.getcwd()\n",
        "print(\"HOME:\", HOME)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install SAM2 and dependencies\n",
        "!git clone https://github.com/facebookresearch/segment-anything-2.git\n",
        "%cd {HOME}/segment-anything-2\n",
        "!pip install -e . -q\n",
        "!python setup.py build_ext --inplace\n",
        "\n",
        "# 필수 라이브러리 설치\n",
        "!pip install -q supervision[assets] jupyter_bbox_widget\n",
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "uN6YzVhitlYx",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download SAM2 checkpoints\n",
        "!mkdir -p {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt -P {HOME}/checkpoints\n",
        "\n",
        "# yolo11s best model\n",
        "!gdown 1A6_iThtWBlzgvGqOP9QXuDYGFL809jYL"
      ],
      "metadata": {
        "id": "1DRz2Ljxtlc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 imports\n",
        "import cv2\n",
        "import torch\n",
        "import base64\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "\n",
        "from pathlib import Path\n",
        "from supervision.assets import download_assets, VideoAssets\n",
        "from sam2.build_sam import build_sam2_video_predictor\n",
        "\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "IS_COLAB = True\n",
        "\n",
        "if IS_COLAB:\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n",
        "\n",
        "from jupyter_bbox_widget import BBoxWidget"
      ],
      "metadata": {
        "id": "xR62T-q5tvQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 모델 초기화  "
      ],
      "metadata": {
        "id": "PIZIN3OC2Po-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: This code enables mixed-precision computing for faster deep learning. It uses bfloat16 for most calculations and, on newer NVIDIA GPUs, leverages TensorFloat-32 (TF32) for certain operations to further boost performance.\n",
        "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "\n",
        "if torch.cuda.get_device_properties(0).major >= 8:\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "CHECKPOINT = f\"{HOME}/checkpoints/sam2_hiera_large.pt\"\n",
        "CONFIG = \"sam2_hiera_l.yaml\"\n",
        "\n",
        "# sam2 model 초기화\n",
        "sam2_model = build_sam2_video_predictor(CONFIG, CHECKPOINT)  # SAM2 모델 빌드 및 가중치로드"
      ],
      "metadata": {
        "id": "Zt5jOlm0tlhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 비디오 정보 확인\n",
        "- fps, 프레임 수, 비디오 길이 출력"
      ],
      "metadata": {
        "id": "VBVjJUUBLZk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/content/test video 07.mp4\" # 사용할 비디오 경로\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # 원본 총 프레임 개수\n",
        "duration = frame_count / fps  # 계산된 비디오 길이\n",
        "cap.release()\n",
        "\n",
        "print(f\"비디오 FPS: {fps:.6f}\")\n",
        "print(f\"비디오 총 프레임 개수: {frame_count}\")\n",
        "print(f\"계산된 비디오 길이: {duration:.2f}초\")"
      ],
      "metadata": {
        "id": "pYwm2saaEwx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 비디오 -> 프레임 분할 및 저장  \n",
        "- sv.get_video_frames_generator(SOURCE_VIDEO, start=0, end=None) 함수를 사용  \n",
        "- 1초에 몇 개의 프레임이 저장되었는지 확인 가능  "
      ],
      "metadata": {
        "id": "As4SLP9wDVrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SCALE_FACTOR = 0.5  # 이미지 크기 50% 축소\n",
        "SOURCE_VIDEO = \"/content/test video 07.mp4\"\n",
        "SOURCE_FRAMES = Path(HOME) / Path(SOURCE_VIDEO).stem\n",
        "SOURCE_FRAMES.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "frames_generator = sv.get_video_frames_generator(SOURCE_VIDEO, start=0, end=None)\n",
        "\n",
        "images_sink = sv.ImageSink(\n",
        "    target_dir_path=SOURCE_FRAMES.as_posix(),\n",
        "    overwrite=True,\n",
        "    image_name_pattern=\"{:05d}.jpg\"\n",
        ")\n",
        "\n",
        "with images_sink:\n",
        "    for frame in frames_generator:\n",
        "        frame = sv.scale_image(frame, SCALE_FACTOR)\n",
        "        images_sink.save_image(frame)\n",
        "\n",
        "# 첫번째 프레임 및 결과 및 저장 경로\n",
        "SOURCE_IMAGE = SOURCE_FRAMES / \"00000.jpg\"\n",
        "TARGET_VIDEO = Path(HOME) / f\"{Path(SOURCE_VIDEO).stem}-result-07.mp4\""
      ],
      "metadata": {
        "id": "_GusUAIZCz1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 프레임 압축 및 다운로드\n",
        "folder_path = \"/content/test video 07\"          # 저장된 프레임이 있는 폴더\n",
        "zip_path = \"/content/test_video_frames_07.zip\"  # 압축 파일명\n",
        "\n",
        "shutil.make_archive(zip_path.replace(\".zip\", \"\"), 'zip', folder_path)\n",
        "files.download(zip_path)"
      ],
      "metadata": {
        "id": "SAQ1-_ts3JIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. SAM2 모델 초기화 (inference 대비용)  \n",
        "- NOTE: 만약 inference_state를 사용하여 이전에 **트래킹(Tracking, 추적)**을 실행했다면, 먼저 reset_state()를 호출하여 초기화해야 함  "
      ],
      "metadata": {
        "id": "6jHLPJUUG4UP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sam2 모델 초기화\n",
        "inference_state = sam2_model.init_state(video_path=SOURCE_FRAMES.as_posix())\n",
        "sam2_model.reset_state(inference_state)"
      ],
      "metadata": {
        "id": "YvHKatbQbkX1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}